{
    "componentChunkName": "component---src-templates-docs-docs-template-jsx",
    "path": "/docs/03/09/2021/LearningPath-Statistical-Modelling-3",
    "result": {"data":{"mdx":{"id":"6b82a80e-0316-5d32-b005-105aad0fecb4","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"type\": \"docs\",\n  \"author\": [\"Dataviz Team\", \"Jean Russell\"],\n  \"title\": \"Statistical Testing\",\n  \"description\": \"Statistical Modeling Part 3 - Regression models\",\n  \"date\": \"2021-09-03T00:00:00.000Z\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar Link = makeShortcode(\"Link\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", {\n    \"id\": \"introduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#introduction\",\n    \"aria-label\": \"introduction permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Introduction\"), mdx(\"p\", null, \"Much of modern research is interested in the relationship between variables.\\nA statistical model is a formal representation of this relationship that lets us test a relationship or predict the value of an unknown based on the value of a known quantity.\"), mdx(\"p\", null, \"The purpose for which the model is created and the data used will significantly influence how the model is interpreted.\\nExperimental scientists may assume a causal relationship between the variables, for example, plant growth rate based on the application of fertilises.\\nOthers may be interested in the relationship between variables; how one may vary given changes in the other.\\nFor example, a meteorologist may predict how much precipitation a region may get based on atmospheric pressure.\"), mdx(\"p\", null, \"There are numerous statistical models; they can be as simple as fitting a regression line between two continuous variables or as complex as a weather forecast, which require a high specification supercomputer to run.\\nIn all cases, models attempt to capture changes in a variable based on known relationships.\\nNo model can do this perfectly. A statistical model is just an assumption.\\nIn all instances, there will be unexplainable variation or \\u2018noise\\u2019 accompanying our models.\\nDavid Spiegelhalter describes models as maps of terrain.\\nNot an image of the terrain.\\nAn important distinction, too often, models are not shown the proper amount of scepticism they deserve.\\nWith this in mind, let\\u2019s look at some powerful yet simple statistical models used frequently in modern research.\"), mdx(\"h2\", {\n    \"id\": \"prerequisites\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#prerequisites\",\n    \"aria-label\": \"prerequisites permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Prerequisites\"), mdx(\"div\", {\n    className: \"bg-white shadow-md p-3 md:p-5 rounded-3xl text-base text-black border-1 border-gray-50\"\n  }, mdx(\"div\", null, mdx(\"span\", {\n    className: \"py-1 px-2 bg-shefYellow text-black hover:bg-yellow-400 transition duration-300 cursor-pointer text-base font-semibold rounded-md mr-3\"\n  }, \"Required\"), mdx(\"p\", {\n    className: \"mt-2\"\n  }, mdx(Link, {\n    to: \"/docs/18/03/2021/LearningPath-Statistical-Modeling-1\",\n    mdxType: \"Link\"\n  }, \"Probability Distributions\"))), mdx(\"div\", {\n    className: \"mt-6\"\n  }, mdx(\"span\", {\n    className: \"py-1 px-2 text-white bg-shefGreen hover:bg-green-700 transition duration-300 cursor-pointer text-base font-semibold rounded-md mr-3\"\n  }, \"Recommended\"), mdx(\"p\", {\n    className: \"mt-2\"\n  }, \"Mean, Median, Mode, Range, Standard Deviation, Z-Score\")), mdx(\"div\", {\n    className: \"mt-6\"\n  }, mdx(\"span\", {\n    className: \"py-1 px-2 text-white bg-shefBlue hover:bg-blue-900 transition duration-300 cursor-pointer text-base font-semibold rounded-md mr-3\"\n  }, \"Optional\"), mdx(\"p\", {\n    className: \"mb-0 mt-2\"\n  }, \"None\"))), mdx(\"h2\", {\n    \"id\": \"regression\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#regression\",\n    \"aria-label\": \"regression permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Regression\"), mdx(\"p\", null, \"Linear models, as the name suggests, assumes a linear relationship between one or more independent variables and a single dependent variable. Linear regression is a useful and very popular statistical modelling tool.\\nIt lets us investigate relationships between variables and make predictions about the value of the dependent variable given the value of the independent variable or variables.\\nThe three most common linear regression models you\\u2019re likely to encounter are simple Linear regression, multiple linear regression, and Logistic regression.\"), mdx(\"h3\", {\n    \"id\": \"simple-linear-regression\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#simple-linear-regression\",\n    \"aria-label\": \"simple linear regression permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Simple linear regression\"), mdx(\"p\", null, \"Simple linear regression is used on occasions with just two continuous variables. A straight line is fit through the data, which represents the linear relationship between the two variables.\\nThere are a number of ways to estimate this linear relationship; by far, the most common method is ordinary least squares. This method seeks to fit a line that minimises the sum of the squared residuals.\\nA residual is the vertical distance from the observed value to the regression line - or, specifically, a measure of the noise between the actual points and the line. That may sound a little alien; it\\u2019s easiest to understand when it\\u2019s presented graphically. So let\\u2019s jump into a case study.\"), mdx(\"h4\", {\n    \"id\": \"example\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#example\",\n    \"aria-label\": \"example permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Example\"), mdx(\"p\", null, \"In 1994 Avery and Burkhart used linear regression to predict whether the growth of a tree trunk (change in basal width) could be used to predict the change in total volume of the trees canopy (crown volume).\\nFigure 1 shows a scatter plot of crown volume against basal growth with a fitted regression line. The residuals are shown as vertical red lines.\\nEach data point has an associated residual; it is the sum of these squared values that the least-squares regression method seeks to minimise.\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/cf34660b12f6660f7265996290827479/Trees2.png\",\n    \"alt\": \"Linear Regression\"\n  }), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Figure 1 - Change in basal width and crown volume with a fitted least-squares regression line.\")), mdx(\"h4\", {\n    \"id\": \"what-does-this-model-tell-us\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#what-does-this-model-tell-us\",\n    \"aria-label\": \"what does this model tell us permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"What does this model tell us?\"), mdx(\"p\", null, \"So we have a fitted model. But what does it tell us? In Figure 1, I\\u2019ve shown you a fitted regression line on a graph, many with be familiar with this.\\nHowever, as we know, a model is a mathematical representation of the relationship between the two variables.\\nTwo coefficients can summarise this line: the slope of the line and the value on the y axis where the line intercepts.\\nTable 1 shows a typical output from a linear regression model.\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Estimate\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Std. Error\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Z value\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Pr\", \"(\", \">\", \"|\", \"z\", \"|\", \")\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"(Intercept)\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"-0.1544\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.1256\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"-1.229\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.224\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Basal Growth\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"38.7929\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"2.8232\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"13.741\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"<2e-16\")))), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Table 1 - Output from a simple linear regression model\")), mdx(\"p\", null, \"We can use the gradient of the regression line to make assumptions about the value of a dependent variable based on the independent variable.\\nIn our example, this is the change in crown volume of our trees, depending on the change in basal growth. You may see the gradient of the regression line referred to as the regression coefficient.\\nIn our example, the regression coefficient is 38.8, so for an increase of 1m\", mdx(\"sup\", null, \"2\"), \" in basal growth, we could expect to see an increase of 38.8 m\", mdx(\"sup\", null, \"3\"), \" in crown volume. The interpretation of this gradient does vary depending on the study.\\nFor example, if we were to assume a causal relationship, this gradient would instead represent the change in crown volume if we increased the basal growth by 1m\", mdx(\"sup\", null, \"2\"), \" in a controlled experiment.\\nOf course, this does not apply here as we can\\u2019t control this change.\"), mdx(\"p\", null, \"As we discussed at the start of this post, models are not perfect.\\nYou can see from Figure 1 that the points are scattered around the regression line.\\nThe model lets us estimate the change in crown volume based on the change in basal growth; it does not provide the exact value.\"), mdx(\"h3\", {\n    \"id\": \"model-assumptions\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#model-assumptions\",\n    \"aria-label\": \"model assumptions permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Model assumptions\"), mdx(\"p\", null, \"When using regression techniques, it\\u2019s important to remember that there is are a list of assumptions that the mathematical model makes about our data.\\nThese assumptions are listed in a helpful handbook for commonly used statistical tests, which can be found \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf\"\n  }, \"here.\"), \"\\nEach of the assumptions below are preceeded by the assumption that you have a good understanding of your independent variables.\"), mdx(\"h4\", {\n    \"id\": \"independent-observations\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#independent-observations\",\n    \"aria-label\": \"independent observations permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), mdx(\"em\", {\n    parentName: \"h4\"\n  }, \"Independent observations\")), mdx(\"p\", null, \"This assumption states that there should be no correlation between successive values; data shouldn\\u2019t have an underlying link or cluster.\\nFor example, data collected over time may have an underlying seasonal component. We can test the independence of observations with a Durbin Watson test.\"), mdx(\"h4\", {\n    \"id\": \"residuals-should-be-normally-distributed\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#residuals-should-be-normally-distributed\",\n    \"aria-label\": \"residuals should be normally distributed permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), mdx(\"em\", {\n    parentName: \"h4\"\n  }, \"Residuals should be normally distributed\")), mdx(\"p\", null, \"The residuals of our model are normally distributed. This is an interesting one, often you will find it stated that the data should be normally distributed.\\nThis is not the case. A perfectly fitting linear model would still have variation in the dependent variable. This is the variation due to the model.\\nWhen this variation from the model is removed, you are left with the residuals.\\nIt is the residuals of the model that should be normally distributed. The residual values are the observed values minus the predicted values from the model.\\nThere are a number of ways we might check for normality of our residuals, we can do this graphically, by plotting them as a histogram and looking for a characteristic bell shape (Figure 2), or by using a Q-Q (quantile - quantile) plot (Figure 3)\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/5518ec40150f4a9112073372f100f9ff/HistResid1.png\",\n    \"alt\": \"Histogram\"\n  }), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Figure 2 - A histogram of residuals\")), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/6c3da14046f46ba2f2d37ae92f4fbd38/qqresi.png\",\n    \"alt\": \"qqplot\"\n  }), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Figure 3 - A quantile-quantile plot of residuals\")), mdx(\"h4\", {\n    \"id\": \"a-linear-relationship-between-the-dependent-variable-and-the-independent-variable\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#a-linear-relationship-between-the-dependent-variable-and-the-independent-variable\",\n    \"aria-label\": \"a linear relationship between the dependent variable and the independent variable permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), mdx(\"em\", {\n    parentName: \"h4\"\n  }, \"A linear relationship between the dependent variable and the independent variable\")), mdx(\"p\", null, \"This is relatively self-explanatory; you can investigate this simply with a scatter plot. You are likely to be familiar with the cigar shape of variables that have a linear relationship.\"), mdx(\"h4\", {\n    \"id\": \"homoscedasticy\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#homoscedasticy\",\n    \"aria-label\": \"homoscedasticy permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), mdx(\"em\", {\n    parentName: \"h4\"\n  }, \"Homoscedasticy\")), mdx(\"p\", null, \"Now, this is a complex word for a simple assumption. Homoscedasticity means \\u2018equal scatter\\u2019. In terms of our model, this means that our residuals should have no systematic change.\\nWe can check this by plotting our residuals against our predicted values. A strong pattern would be a cause for concern.\"), mdx(\"h4\", {\n    \"id\": \"no-outlying-variables-that-have-a-substantial-influence-on-the-fit\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#no-outlying-variables-that-have-a-substantial-influence-on-the-fit\",\n    \"aria-label\": \"no outlying variables that have a substantial influence on the fit permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), mdx(\"em\", {\n    parentName: \"h4\"\n  }, \"No outlying variables that have a substantial influence on the fit\")), mdx(\"p\", null, \"Outlying variables can have a big impact on our linear models as whatever method we use to fit our line will try to reduce a very large residual error.\\nSo just a few outlying data points can have an unduly large influence on our regression line.\\nThese outliers are typically when the independent variable value is exceptionally large or small compared to other values in the data set.\\nTo check that there are no outlying variables that are influencing the position and slope of our least-squares line, we can look at Cook\\u2019s distance and leverage for each point.\\nFor more information about Cook's distance and levarge points I can reccomend \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.statology.org/how-to-identify-influential-data-points-using-cooks-distance/\"\n  }, \"this article.\")), mdx(\"h2\", {\n    \"id\": \"multiple-linear-regression\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#multiple-linear-regression\",\n    \"aria-label\": \"multiple linear regression permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Multiple linear regression\"), mdx(\"p\", null, \"As you might imagine, multiple linear regression is used in those instances where there is more than one independent variable.\\nThe principles are remarkably similar to that of simple linear regression. However, we have more information! We can use multiple linear regression to improve our models by investigating how multiple independent variables influence our dependent variable.\"), mdx(\"p\", null, \"A classic example of multiple regression is how different factors affect the birth weight of a baby.\\nIn our example, the weight of the baby will be our dependent variable. The duration of gestation and the height of the mother will be our independent variables.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"Let\\u2019s start by plotting our data. Nothing fancy, we\\u2019ll just take a look.\"), mdx(\"p\", null, \"We can see that birthweight and gestation are correlated; gestation is likely to be a good predictor.\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"The relationship isn\\u2019t as strong between birth weight and the height of the mother. It may still be a valuable piece of information.\\nWhen we run our linear model to include the second independent variable, the same principles apply.\\nThe least-squares model finds the best mathematical representation of the relationship by fitting a plane to our data, and attempting to minimise the error.\\nAnd just like linear regression, the model returns a table of outputs, summarising the model.\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Estimate\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Std. Error\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Z value\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Pr\", \"(\", \">\", \"|\", \"z\", \"|\", \")\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"(Intercept)\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"-6.025\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"1.759\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"-3.452\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.00146\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Gestation\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.151\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.025\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"6.015\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"4.94e-07\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Mother\\u2019s Height\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.021\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.010\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"2.037\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.04853\")))), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Table 2 - Output from Multiple linear regression\")), mdx(\"p\", null, \"The interpretation of the results is a little different. Once again, we have an intercept, which tells us where our plane intersects the y-axis.\\nThe slope coefficient for each of the independent variables estimates the change in our dependent variable for every unit change in the independent.\\nIn our example, our model tells us that there is a 0.151kg increase in birthweight for every additional week of gestation.\\nFor every additional centimetre in the mother\\u2019s height, an additional 0.021kg increase in birthweight can be expected.\"), mdx(\"p\", null, \"You will note that each of the independent variables also has p value. This column tells us whether each of the independent variables are significant predictors.\\nFor example, in our first instance, the value tells us whether the complex model, using both Gestation length and Mother\\u2019s height, is more effective than a model that only uses mother\\u2019s height.\\nThe significant p value tells that it most certainly is more effective to use both variables.\"), mdx(\"p\", null, \"On a final note, multiple linear regression is often used when a researcher would like to adjust for the influence other variables, often called confounders.\\nThis can be a difficult concept to grasp; a confounder is another variable that may influence the value of the dependent variable and the independent variables within the study.\\nFor example, let\\u2019s say you were studying the number of people getting sunburnt. You found that the rate of ice cream consumption is significantly correlated to the number of sunburns.\\nHowever, of course, warmer weather will influence both ice cream consumption and the number of sunburns.\\nTherefore hot weather is certainly a confounding variable if we were modelling the relationship between ice cream consumption and sunburns.\\nThis is a silly example, of course, but it demonstrates a very important point. Controlling for these confounders comes with experience and a good deal of subject knowledge.\"), mdx(\"h4\", {\n    \"id\": \"dummy-variables\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#dummy-variables\",\n    \"aria-label\": \"dummy variables permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Dummy variables\"), mdx(\"p\", null, \"When performing multiple linear regression, like simple linear regression, our dependent variable must be continuous.\\nHowever, our independent variables can be expanded to include categorical variables. They do require some special preparation.\\nTo use a categorical variable, we must recode them into dummy variables. I won\\u2019t go into this here, but see \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stattrek.com/multiple-regression/dummy-variables.aspx\"\n  }, \"this Stat Trek post\"), \" for more information about how to prepare categorical variables and include them in multiple linear regression models.\"), mdx(\"h3\", {\n    \"id\": \"model-assumptions-1\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#model-assumptions-1\",\n    \"aria-label\": \"model assumptions 1 permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Model Assumptions\"), mdx(\"p\", null, \"All of the assumptions that are made for simple linear regression apply to multiple linear regression, with the additional assumption that none of the independent variables in the model should be highly related. This can be tested very easily by checking that none are highly correlated.\"), mdx(\"h2\", {\n    \"id\": \"logistic-regression\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#logistic-regression\",\n    \"aria-label\": \"logistic regression permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Logistic regression\"), mdx(\"p\", null, \"Logistic regression is used when the dependent variable is binary; that is, a categorical variable with only two values.\\nThink of situations like yes or no, true or false and so forth. We can use logistic regression to predict the probability that a value will be one of two categories based on the value of an independent variable.\"), mdx(\"p\", null, \"Why can\\u2019t we just use linear regression? There several of reasons why linear regression isn\\u2019t appropriate for a binary dependant variable.\\nThe most prominent is that linear regression could predict a value that is greater than true or less than false. And this is, of course, impossible.\\nLogistic regression, on the other hand, takes into account these limits. The sketch below shows how linear regression may lead us to make impossible assumptions with our model, while logistic regression fits an S-shaped curve to our data.\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/91b952e765d183248a9428bf50bb8fa1/Axis.png\",\n    \"alt\": \"Linear Vs Logistic\"\n  })), mdx(\"h4\", {\n    \"id\": \"example-1\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#example-1\",\n    \"aria-label\": \"example 1 permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"Example\"), mdx(\"p\", null, \"This may sound strange, so let\\u2019s think of a case study. We\\u2019ll use R\\u2019s popular \\u2018Motor Trend Car Road Tests\\u2019 dataset to look at the relationship between engine shape and the weight of the vehicle.\\nOur binary dependent variable will be engine shape, specifically, \\u2018v-shaped\\u2019 and \\u2018straight\\u2019. If a vehicle has a v-shaped (vs) engine, it will be coded with a value of 0, and if it has a straight engine, it will have a value of 1.\\nThe independent variable for this example is weight in 1000lbs (forgive the imperial measures, it\\u2019s an American dataset). Figure 4 shows the two varibales with a fitted logistic regression line.\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/a47ffe9d94312ab192d60ef537f2f9e4/Engines.png\",\n    \"alt\": \"Logistic\"\n  }), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Figure 4 - Engine shape and car weight with a fitted logistic regression line.\")), mdx(\"h4\", {\n    \"id\": \"what-does-this-model-tell-us-1\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h4\",\n    \"href\": \"#what-does-this-model-tell-us-1\",\n    \"aria-label\": \"what does this model tell us 1 permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"undefined\", {\n    parentName: \"a\"\n  })), \"What does this model tell us?\"), mdx(\"p\", null, \"So we have our model, represented graphically above, but what can we do with it. Logistic regression is interpreted differently to linear regression.\\nAs we mentioned above the curve represents the probability that a car has straight shaped engine based on the weight of the car.\\nSo there is a high probability that a light car, weighing under 2000lbs, will have a straight engine. Whereas, there is a small probability that a car that weighs over 5000lbs will have a straight engine.\\nThe probability of a car weighing approximately 3000lbs, having a straight engine is approximately 50%.\"), mdx(\"p\", null, \"As we know from linear regression, the line on our chart is just a representation of a mathematical product.\\nJust like with linear regression, we can produce a table of coefficients for our logistic regression model. Table 3 shows our summary.\\nYou\\u2019ll see that, just like linear regression, we have an intercept value and a coefficient of the slope of the line.\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Estimate\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Std. Error\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Z value\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Pr\", \"(\", \">\", \"|\", \"z\", \"|\", \")\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"(intercept)\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"5.7147\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"2.3014\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"2.483\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.01302\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Weight\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"-1.9105\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.7279\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"-2.625\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.00867\")))), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Table 3 - Typical output from logistic regression\")), mdx(\"p\", null, \"But how do we get these if we have an S-shaped line?\\nWell, while Figure 4 above is what we typically associate with logistic regression, these coefficients are calculated by transforming the y axis from probability to the log of the odds.\\nDoing so turns our S-shaped line into a straight line that is symmetrical around 0. Odds are simply the ratio of something happening, to something not happening.\\nIn our example, the odds of having a straight engine, over the odds of having a VS engine.\\nIn logistic regression, we use the log of these odds as it creates a more symmetrical distribution about 0.\\nI won\\u2019t show you the equation, but if you\\u2019d like to learn more about the logit function and how probability is converted into log(odds) then I can recommend this excellent \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.youtube.com/watch?v=ARfXDSkQf1Y\"\n  }, \"StatQuest video.\")), mdx(\"p\", null, \"In our example, we can use these coefficients to make assumptions about our data.\\nThe intercept is the value on the y axis when the weight of the vehicle is 0. So if we have a vehicle that weighs nothing, the log(odds of a straight shaped engine) is 5.7.\\nAs with linear regression, the coefficient of the slope of the line tells us what is likely to happen with every unit change in our independent variable.\\nIn this instance, for every unit change in vehicle weight, the log(odds of a straight shaped engine) is -1.9.\"));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#introduction","title":"Introduction"},{"url":"#prerequisites","title":"Prerequisites"},{"url":"#regression","title":"Regression","items":[{"url":"#simple-linear-regression","title":"Simple linear regression","items":[{"url":"#example","title":"Example"},{"url":"#what-does-this-model-tell-us","title":"What does this model tell us?"}]},{"url":"#model-assumptions","title":"Model assumptions","items":[{"url":"#independent-observations","title":"Independent observations"},{"url":"#residuals-should-be-normally-distributed","title":"Residuals should be normally distributed"},{"url":"#a-linear-relationship-between-the-dependent-variable-and-the-independent-variable","title":"A linear relationship between the dependent variable and the independent variable"},{"url":"#homoscedasticy","title":"Homoscedasticy"},{"url":"#no-outlying-variables-that-have-a-substantial-influence-on-the-fit","title":"No outlying variables that have a substantial influence on the fit"}]}]},{"url":"#multiple-linear-regression","title":"Multiple linear regression","items":[{"items":[{"url":"#dummy-variables","title":"Dummy variables"}]},{"url":"#model-assumptions-1","title":"Model Assumptions"}]},{"url":"#logistic-regression","title":"Logistic regression","items":[{"items":[{"url":"#example-1","title":"Example"},{"url":"#what-does-this-model-tell-us-1","title":"What does this model tell us?"}]}]}]},"fields":{"slug":"/docs/03/09/2021/LearningPath-Statistical-Modelling-3","readingTime":{"text":"15 min read"},"slugOrigin":"/2021-09-03-LearningPath-Statistical-Modelling-3/"},"frontmatter":{"type":"docs","title":"Statistical Testing","date":"03 September 2021","description":"Statistical Modeling Part 3 - Regression models","tag":null,"category":null,"featured":null,"thumbnail":null,"author":[{"name":"Dataviz Team","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/871859109b0659b0d6a48ae540cb439e/780cc/dataviz.png","srcSet":"/static/871859109b0659b0d6a48ae540cb439e/780cc/dataviz.png 214w","sizes":"100vw"},"sources":[{"srcSet":"/static/871859109b0659b0d6a48ae540cb439e/6c5cc/dataviz.webp 214w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9158878504672897}}}},{"name":"Jean Russell","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/0241dc4c544ec54fee0c378b40ca6bc6/0be83/default.png","srcSet":"/static/0241dc4c544ec54fee0c378b40ca6bc6/f054e/default.png 750w,\n/static/0241dc4c544ec54fee0c378b40ca6bc6/ae1c8/default.png 1080w,\n/static/0241dc4c544ec54fee0c378b40ca6bc6/0be83/default.png 1280w","sizes":"100vw"},"sources":[{"srcSet":"/static/0241dc4c544ec54fee0c378b40ca6bc6/4f03f/default.webp 750w,\n/static/0241dc4c544ec54fee0c378b40ca6bc6/4f506/default.webp 1080w,\n/static/0241dc4c544ec54fee0c378b40ca6bc6/9e21f/default.webp 1280w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}],"disableTOC":null,"d3":null}}},"pageContext":{"id":"6b82a80e-0316-5d32-b005-105aad0fecb4","slug":"/docs/03/09/2021/LearningPath-Statistical-Modelling-3","next":{"node":{"id":"15792cbb-88f3-5d34-a085-2e0bed94312a","fields":{"slug":"/docs/21/07/2021/Contribute-visualisation"},"frontmatter":{"template":null,"title":"Contribute visualisation","date":"Wednesday 21st July 2021","category":null,"tag":null,"d3":null,"type":"docs","published":null}}}}},
    "staticQueryHashes": ["160810946","3815917638","3976606562"]}